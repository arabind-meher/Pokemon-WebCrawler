# ğŸ•¸ï¸ PokÃ©mon WebCrawler

A Scrapy-based web crawler for extracting detailed information about PokÃ©mon from [pokemondb.net](https://pokemondb.net). This project scrapes structured data including forms, types, stats, and more, and stores it in a local MongoDB database.

---

## ğŸ“¦ Features

- Crawls PokÃ©mon data starting from a **single PokÃ©mon page**
- Extracts structured data:
  - Name, Form, Types, Species
  - PokÃ©dex Index (National and Regional)
  - Height, Weight, Abilities
  - Training and Breeding Information
  - Base Stats and Total Stats
- Stores data in MongoDB (`pokemon_db.pokedex`)
- Uses `.env` configuration for secure DB connection
- Modular codebase with clean separation (spider, pipeline, DB client)

---

## ğŸš€ Technologies Used

- Python 3.12+
- [Scrapy](https://scrapy.org/)
- [MongoDB](https://www.mongodb.com/)
- [pymongo](https://pypi.org/project/pymongo/)
- [BeautifulSoup4](https://pypi.org/project/beautifulsoup4/)
- [python-dotenv](https://pypi.org/project/python-dotenv/)

---

## ğŸ“‚ Project Structure

```
pokemon_scraper/
â”œâ”€â”€ spiders/
â”‚   â””â”€â”€ pokemon_spider.py        # Scrapes PokÃ©mon data
â”œâ”€â”€ items.py                     # Defines the data structure
â”œâ”€â”€ pipelines.py                 # Handles MongoDB insertion
â”œâ”€â”€ connection.py                # MongoDB client using .env
â”œâ”€â”€ middlewares.py              # Default Scrapy middlewares
â”œâ”€â”€ settings.py                  # Scrapy configuration
.env                             # MongoDB connection settings (not committed)
```

---

## âš™ï¸ Setup Instructions

### 1. Clone the repository
```bash
git clone https://github.com/your-username/Pokemon-WebCrawler.git
cd Pokemon-WebCrawler
```

### 2. Create a virtual environment
```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install dependencies
```bash
pip install -r requirements.txt
```

### 4. Add MongoDB configuration to `.env` file
Create a `.env` file in the project root:
```env
MONGO_URI=mongodb://localhost:27017/
MONGO_DB_NAME=pokemon_db
```

### 5. Run MongoDB
Ensure MongoDB is running locally on `localhost:27017`.

### 6. Start the crawler
```bash
scrapy crawl pokemon
```

---

## ğŸ“Š Crawl Summary

- **Total Pages Crawled**: 1,025  
- **Total PokÃ©mon Entries Extracted**: 1,215 (including alternate forms)

---

## ğŸ§ª Sample Output (MongoDB document)

```json
{
  "name": "Bulbasaur",
  "form": null,
  "index": 1,
  "types": ["Grass", "Poison"],
  "species": "Seed PokÃ©mon",
  "height": "0.7 m",
  "weight": "6.9 kg",
  "abilities": ["Overgrow", "Chlorophyll"],
  "local_index": {
    "Kanto": 1
  },
  "training": {
    "EV yield": "1 Special Attack",
    "Base EXP": "64"
  },
  "breeding": {
    "Egg group": "Monster, Grass"
  },
  "base_stats": {
    "HP": 45,
    "Attack": 49,
    "Defense": 49,
    "Total": 318
  }
}
```

---

## ğŸ“Œ Notes

- The crawler currently starts from a **manually specified PokÃ©mon page** (e.g., `https://pokemondb.net/pokedex/wo-chien`).
- Crawling the full PokÃ©dex using `/pokedex/all` is not yet implemented.
- All PokÃ©mon forms are stored separately â€” no deduplication is performed.

---

## ğŸ™‹â€â™‚ï¸ Author

**Arabind Meher**  
- [GitHub](https://github.com/arabind-meher)  
- [LinkedIn](https://www.linkedin.com/in/arabind-meher/)
